# LLM Inference

## LLM Inference Optimization

- 24_Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads [[paper]](Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads)
- 24_Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving [[paper]](https://arxiv.org/pdf/2407.00079)
- 24_OSDI_DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving [[paper]](https://arxiv.org/abs/2401.09670)
- 24_ISCA_Splitwise: Efficient Generative LLM Inference Using Phase Splitting [[paper]](https://arxiv.org/abs/2311.18677)