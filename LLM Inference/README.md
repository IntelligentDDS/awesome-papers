# LLM Inference

- A Survey on Efficient Inference for Large Language Models [[paper]](https://arxiv.org/abs/2404.14294)

## LLM Inference Optimization
- 24_Characterizing and Efficiently Accelerating Multimodal Generation Model Inference [[paper]](https://arxiv.org/abs/2410.00215)
- 24_MLSys_Punica: Multi-Tenant LoRA Serving [[paper]](https://arxiv.org/abs/2310.18547) [[code]](https://github.com/punica-ai/punica)
- 24_SC_LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services [[paper]](https://arxiv.org/abs/2410.02425)
- 24_Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads [[paper]](https://arxiv.org/abs/2401.11181)
- 24_Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving [[paper]](https://arxiv.org/pdf/2407.00079)
- 24_OSDI_DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving [[paper]](https://arxiv.org/abs/2401.09670)
- 24_OSDI_Llumnix: Dynamic Scheduling for Large Language Model Serving [[paper]](https://www.usenix.org/system/files/osdi24-sun-biao.pdf)
- 24_ISCA_Splitwise: Efficient Generative LLM Inference Using Phase Splitting [[paper]](https://arxiv.org/abs/2311.18677)
- 24_OSDI_Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve [[paper]](https://arxiv.org/pdf/2403.02310.pdf)
- 24_Fast Distributed Inference Serving for Large Language Models [[paper]](https://arxiv.org/pdf/2305.05920.pdf)
- 24_OSDI_ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models [[paper]](https://arxiv.org/abs/2401.14351) [[code]](https://github.com/ServerlessLLM/ServerlessLLM)
- 24_ICWS_Enabling Efficient Batch Serving for LMaaS via Generation Length Prediction [[paper]](https://arxiv.org/abs/2406.04785)
- 23_Nips_Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline [[paper]](https://arxiv.org/abs/2305.13144) [[code]](https://github.com/zhengzangw/Sequence-Scheduling)
- 23_ICML_FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU [[paper]](https://dl.acm.org/doi/10.5555/3618408.3619696) [[code]](https://github.com/FMInference/FlexGen)
- 23_SOSP_Efficient Memory Management for Large Language Model Serving with PagedAttention [[paper]](https://arxiv.org/abs/2309.06180) [[code]](https://github.com/vllm-project/vllm)
- 23_OSDI_AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving [[paper]](https://www.usenix.org/system/files/osdi23-li-zhuohan.pdf)
- 22_NSDI_Orca: A Distributed Serving System for Transformer-Based Generative Models [[paper]](https://www.usenix.org/conference/osdi22/presentation/yu)


## LLM Simulation
- 24_MLSys_Vidur: A Large-Scale Simulation Framework For LLM Inference [[paper]](https://arxiv.org/abs/2405.05465) [[code]](https://github.com/microsoft/vidur)