# LLM Inference

- A Survey on Efficient Inference for Large Language Models [[paper]](https://arxiv.org/abs/2404.14294)

## LLM Inference Optimization

- 24_Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads [[paper]](Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads)
- 24_Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving [[paper]](https://arxiv.org/pdf/2407.00079)
- 24_OSDI_DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving [[paper]](https://arxiv.org/abs/2401.09670)
- 24_ISCA_Splitwise: Efficient Generative LLM Inference Using Phase Splitting [[paper]](https://arxiv.org/abs/2311.18677)
- 24_Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve [[paper]](https://arxiv.org/pdf/2403.02310.pdf)
- 24_Fast Distributed Inference Serving for Large Language Models [[paper]](https://arxiv.org/pdf/2305.05920.pdf)
- 24_OSDI_ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models [[paper]](https://arxiv.org/abs/2401.14351) [[code]](https://github.com/ServerlessLLM/ServerlessLLM)
- 23_Nips_Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline [[paper]](https://arxiv.org/abs/2305.13144) [[code]](https://github.com/zhengzangw/Sequence-Scheduling)
- 23_ICML_FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU [[paper]](https://dl.acm.org/doi/10.5555/3618408.3619696) [[code]](https://github.com/FMInference/FlexGen)
- 23_SOSP_Efficient Memory Management for Large Language Model Serving with PagedAttention [[paper]](https://arxiv.org/abs/2309.06180) [[code]](https://github.com/vllm-project/vllm)
- 22_NSDI_Orca: A Distributed Serving System for Transformer-Based Generative Models [[paper]](https://www.usenix.org/conference/osdi22/presentation/yu)