# LLM Training

## Training Failure
- 24_NSDI_Characterization of Large Language Model Development in the Datacenter [[paper]](https://www.usenix.org/system/files/nsdi24-hu.pdf)
- 24_Revisiting Reliability in Large-Scale Machine Learning Research Clusters [[paper]](Revisiting Reliability in Large-Scale Machine Learning Research Clusters)
- 24_ATC_SuperBench: Improving Cloud AI Infrastructure Reliability with Proactive Validation [[paper]](https://arxiv.org/abs/2402.06194)  [Microsoft] [Best Paper]
- 24_NSDI_MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs [[paper]](https://www.usenix.org/system/files/nsdi24-jiang-ziheng.pdf)
- 24_The Llama 3 Herd of Models [[paper]](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) [Meta]
  > During a 54-day snapshot period of pre-training, we experienced a total of 466 job interruptions.
- 23_Unicron: Economizing Self-Healing LLM Training at Scale [Alibaba] [[paper]](https://arxiv.org/abs/2401.00134) 


## Checkpoint

- 24_Eurosys_Just-In-Time Checkpointing: Low Cost Error Recovery from Deep Learning Training Failures [[paper]](https://dl.acm.org/doi/pdf/10.1145/3627703.3650085)
  > Most errors during training occur due to failures of a single GPU or network device (either hardware or transient errors), while host/CPU and simultaneous multi-node failures are extremely rare